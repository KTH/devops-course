# Feedback 

By Atheer Salim, Milad Farahani

Number of Words: 608
## Code of Conduct 

**I/We certify that generative AI, incl. ChatGPT, has not been used to write this feedback. Using generative AI without permission is considered academic misconduct.**

## High Level Overview

### Strengths
- The presentation covers critical issues about TinyMLOps that are highly relevant in the current day and age.
- The structure was well-thought-out, with coverage of relevant aspects of   TinyMLOps.
- Good introduction with an engaging aspect of humor.
- Inclusion of technical part regarding TinyMLOps.
- There is a good emphasis on practical implications which offers insight into real-world applications.
- Excellent speech and tone.
- Great illustrations
- Good Take-Home Message


### Weaknesses
- Wrong date on the intro slides.
- The slides contain the name and slide number to the bottom right which is not accurate with the current slide.
- Clearly distinguish the content included in the paper you are presenting from content taken from other papers.
- Talk a bit more about the related papers in terms of their achievement. 

## Additional Material

The paper [Implementing MLOps on Edge-Cloud Systems: A New Paradigm for Training at the Edge](https://uwspace.uwaterloo.ca/items/bc1f43a4-96dd-44d6-946f-b5cd19660647) proposes a MLOps architecture used to orchestrate edge-cloud model training and synchronization. The model has been tested on IoT devices, this material could be relevant to look at 

The presentation briefly mentioned Federated Learning. A deeper overview of Federated Learning as opposed to Centralized Training could be valuable. One of the many sources providing good insight regarding Federated Learning is Google’s work. They clearly describe how Federated Learning could serve as a reference for understanding how updates can be managed across distributed devices. Here is one of the many sources: [Federated Learning: Collaborative Machine Learning without Centralized Training Data](https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/)

## More Detailed Feedback 

### General
The tone and the speech of the presenters were very good. But just like in machine learning, more training would result in an even better speech :)

The way the presenters covered the need to address challenges in edge AI deployment was done in a good way, making it understandable for the listener.

The technical part was explained in a manner that was coherent for the listener and not overwhelming. There were also readable code snippets included which extended the technical details even further.

### Introduction
In the first few slides of your presentation, it contains the date “21 September 2024” which does not align with the date of week 4 presentation time. Which should be “18 September 2024”

The touch of humor was very refreshing and multiple jokes were funny. Especially the AI standing for "Apple Intelligence" joke. Another entertaining visual was the hand pinching “TinyMLOps” indicating that it is “tiny”.

### Fragmented landscape Slide
When discussing ONNX it would be nice if you could show an example of the exported format that would make it easier to understand and clearly show the benefits it has. 

When you mention Rune, it should be clearly highlighted that this was not presented in the paper you are presenting and that it comes from another paper. Furthermore, the citation for the paper (presenting Rune) is not visible since the text color matches the background, making it hard to see.

### Protecting Models Intellectual Property Slide
Could you elaborate further on what exactly “local access” means when you are talking about direct stealing, does this mean physical access, root access, or what exactly?

### Take Home Message Slide
In the end, it was evident that the take-home message was well-thought-out. It was broad enough for it to cover TinyMLOps but also specific enough to encapsulate the purpose of the paper and its findings. 
